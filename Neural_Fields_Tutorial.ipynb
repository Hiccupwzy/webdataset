{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Fields_Tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR33D7bApGa4"
      },
      "source": [
        "<img src=\"https://www.inovex.de/wp-content/uploads/inovex-logo-dunkelblau-quadrat.png\" width=\"100px\" align=\"left\"/>\n",
        "\n",
        "\n",
        "<table align=\"right\">\n",
        "\n",
        "  <td >\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/inovex/notebooks/blob/main/Neural_Fields_Tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/inovex/notebooks/blob/main/Neural_Fields_Tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1TD9U6Qui7-"
      },
      "source": [
        "# Neural Fields Tutorial\n",
        "\n",
        "Hi there, I am glad you found this notebook and are eager to learn something about Neural Fields! You most certainly found this notebook while reading my introductory blog article about the topic. If not, here is the link to the post. I recommend reading the blog article first, because it discusses the fundamentals of the topics you will see in this notebook. There is aslo the possiblity to work through both in parallel, as I refer back and forth between the documents. This notebook is not meant to be a standalone tutorial, so if questions arise while you read trough this, there will be most certainly an explanation of it in the blog article. If something is unclear, feel free to comment under the blog article. I am happy to answer your questions!\n",
        "\n",
        "This notebook then covers:\n",
        "\n",
        "  1. Training of a simple Neural Field for image representation\n",
        "  2. Fourier Feature Mapping\n",
        "  3. Conditional Neural Field (represented as an Autodecoder)  \n",
        "  4. Global vs Local Conditioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHUWbyu6u6Ap"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsBWK6IpriyC"
      },
      "source": [
        "import os\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from collections.abc import Iterable\n",
        "from typing import List\n",
        "\n",
        "import imageio\n",
        "from skimage.transform import resize\n",
        "from skimage.color import rgba2rgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "!pip install livelossplot --quiet\n",
        "from livelossplot import PlotLosses"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to connect to a GPU runtime. You can do so by clicking on `Runtime` > `Change Runtime type` in the menu above. Simply select `GPU` in the hardware accelerator dropdown menu. The following cell prints you some\n",
        "information about the GPU colab assigned to you. It will fail if you are not connected to a GPU Runtime."
      ],
      "metadata": {
        "id": "yh5mLevnLqZy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2GkqfcDPOC9",
        "outputId": "9d3341d1-4ada-41f4-f778-1b08d96c3ff3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov 27 09:03:22 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amwQFX7zURMq"
      },
      "source": [
        "## 1. Neural Field for a single image\n",
        "\n",
        "\n",
        "In the following, you will train a simple neural field for image representation from scratch. I will recommend reading section 1 of the blog article before you continue with this notebook.\n",
        "\n",
        "**A short TL;DR**: A field is a function that maps coordinates in space and time to a physical quantity. A neural field is a field parameterized by a Neural Network. For images, we have spatial coordinates (pixels), that map to color intensities. If we want to represent an image as a neural field, we want to learn a function $f: \\mathbb{R}^2 \\to \\mathbb{R}^3$, which maps continuous pixel coordinates $(u, v)$ to the three base colors red, green, and blue. This is also visualized in the image below.\n",
        "\n",
        "\n",
        "<img src=\"https://www.inovex.de/wp-content/uploads/coordinate-based-mlp.png\" width=\"500px\" align=\"center\"/>\n",
        "\n",
        "\n",
        "## Data\n",
        "The following cell downloads an image of our inovex logo. We will use this image to train an implicit neural representation of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c34E60KYD8E3",
        "outputId": "723823ea-beb5-4191-ad11-e53771306e87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "image = imageio.imread('https://www.inovex.de/wp-content/uploads/inovex-logo-dunkelblau-quadrat.png')[...]\n",
        "print(image.shape)\n",
        "image = rgba2rgb(resize(image, (512, 512)))\n",
        "plt.imshow(image);"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-648f25309322>:1: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  image = imageio.imread('https://www.inovex.de/wp-content/uploads/inovex-logo-dunkelblau-quadrat.png')[...]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3001, 3001, 3)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "the input array must have size 4 along `channel_axis`, got (512, 512, 3)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-648f25309322>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.inovex.de/wp-content/uploads/inovex-logo-dunkelblau-quadrat.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgba2rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/skimage/color/colorconv.py\u001b[0m in \u001b[0;36mrgba2rgb\u001b[0;34m(rgba, background, channel_axis)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;34mf'got {arr.shape}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         )\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mfloat_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_supported_float_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: the input array must have size 4 along `channel_axis`, got (512, 512, 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWWehBvcF_jI"
      },
      "source": [
        "You have now loaded the image. The `rgba2rgb()` function removes the alpha channel and normalizes the color channels to the range `[0, 1]`. Since we want to represent the image as a continuous function, parameterized by a MLP, it is a good idea to also normalize the spatial coordinates to the unit square.\n",
        "\n",
        "We then split the image in training and test data. Therefore, we use every other pixel as training data sample and the rest as test data. When we want to visualize intermediary results, we take the whole grid and pass it through the network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_ON0UwCE8dV"
      },
      "source": [
        "image = torch.tensor(image, dtype=torch.float32)\n",
        "\n",
        "x = np.linspace(0, 1, image.shape[1], endpoint=False)\n",
        "\n",
        "# sample the grid, which will be the input to the model\n",
        "grid = torch.tensor(np.stack(np.meshgrid(x, x), -1), dtype=torch.float32)\n",
        "X, Y = [grid.view(-1, 2), image.view(-1, 3)]\n",
        "test_X, test_y = [X[1::2], Y[1::2]]\n",
        "train_X, train_y = [X[::2], Y[::2]]\n",
        "\n",
        "test_X.requires_grad = False\n",
        "train_X.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzAr30DJqeMv"
      },
      "source": [
        "\n",
        "### Model definition\n",
        "\n",
        "The following cell defines our model using PyTorch. We will use a very simple MLP Architecture with 4 Layers and 256 Neurons each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnXqbEpEImzZ"
      },
      "source": [
        "class NeuralField(nn.Module):\n",
        "  def __init__(self, hidden_layers=2, neurons_per_layer=256, input_dimension=2):\n",
        "    super().__init__()\n",
        "    self.input_layer = nn.Linear(input_dimension, neurons_per_layer)\n",
        "    self.hidden_layers = nn.ModuleList([nn.Linear(neurons_per_layer, neurons_per_layer) for i in range(hidden_layers)])\n",
        "    self.output_layer = nn.Linear(neurons_per_layer, 3)\n",
        "\n",
        "  def forward(self, input):\n",
        "    x = F.relu(self.input_layer(input))\n",
        "    for layer in self.hidden_layers:\n",
        "      x = F.relu(layer(x))\n",
        "    return torch.sigmoid(self.output_layer(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09AV9-GfvmCn"
      },
      "source": [
        "### Model training\n",
        "\n",
        "Alrighty, it's time to train our model! The following cell defines the loss function and a metric that we will use to assess the reconstruction quality. The cell after contsins the model training loop. You can monitor the training by looking at the loss curve and the peak signal to noise ratio (PSNR). We will also save an temporary image every 25 iterations to visualize the training progress afterwards.\n",
        "\n",
        "\n",
        "For those of you who are not familiar with the PSNR: It quantifies the ratio between the maximum power of a signal and the influence of corrupting noise affecting the preciseness of the signal. It is widely used to quantify the reconstruction quality for images and video subject to lossy compression. It's computation involves the Mean Squared Error between the noise-free image and the lossy approximation. It is expressed as a logarithmic quantity on the decibel scale."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(gt, pred):\n",
        "  return 0.5 * torch.mean((gt - pred) ** 2., (-1, -2)).sum(-1).mean()\n",
        "\n",
        "def psnr(gt, pred):\n",
        "  return -10 * torch.log10(2. * torch.mean((gt - pred) ** 2.))"
      ],
      "metadata": {
        "id": "-VhgLn3OlhwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmWvMV1Fvi_g"
      },
      "source": [
        "model = nn.DataParallel(NeuralField().cuda())\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "images = []\n",
        "\n",
        "liveloss = PlotLosses()\n",
        "for i in range(2000):\n",
        "  model.train()\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  prediction = model(train_X)\n",
        "  loss = mse(train_y.to('cuda'), prediction)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if i % 25 == 0:\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "      reconstruction = model(X).detach().cpu()\n",
        "\n",
        "    liveloss.update({'PSNR train': psnr(train_y, prediction.detach().cpu()),\n",
        "                     'Loss train': mse(train_y, prediction.detach().cpu()),\n",
        "                     'PSNR test': psnr(test_y, reconstruction[::2]),\n",
        "                     'Loss test': mse(test_y, reconstruction[::2])},\n",
        "                    current_step=i)\n",
        "    liveloss.send()\n",
        "    images.append(reconstruction.numpy().reshape(512, 512, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the training has converged, you are probably eager to see the results. The following cell concatenates all the temporary images into a video and displays it to you."
      ],
      "metadata": {
        "id": "Fuh2JtiR4eDO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzVZNu0jCG6c"
      },
      "source": [
        "all_images = np.stack(images)\n",
        "data8 = (255*np.clip(all_images,0,1)).astype(np.uint8)\n",
        "f = os.path.join('training_convergence_no_ff.mp4')\n",
        "imageio.mimwrite(f, data8, fps=20)\n",
        "\n",
        "# Display video inline\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open(f,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f'''\n",
        "<video width=500 controls autoplay loop>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are probably very disappointed by now, and ask yourself where the problem lies. It turns out that Neural Networks have a hard time learning high-frequency details in low dimensional problem domains. For our task at hand, this means that our network struggles learning the sharp edges of the inovex logo (and any other image, for that matter).\n",
        "\n",
        "There is actually a theoretical explanation from [Tancik et al. (2020)](https://bmild.github.io/fourfeat/) for that, which I explain in the blog article. So this is a good time to switch tabs and read section 2 of the article ;)"
      ],
      "metadata": {
        "id": "y6Wsmdbdxd6e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwKF-c820hJQ"
      },
      "source": [
        "model.eval()\n",
        "pred = model(X.to('cuda')).cpu().detach().numpy().reshape(512, 512, 3)\n",
        "f, (ax0, ax1) = plt.subplots(1, 2)\n",
        "\n",
        "ax0.imshow(pred)\n",
        "ax1.imshow(image);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c20FKgcveZt"
      },
      "source": [
        "## 2. Fourier Features for Neural Fields\n",
        "\n",
        "In the following cell, we inplement the Random fourier feature mapping as proposed by [Rahimi & Recht (2007)](https://proceedings.neurips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf), which is defined as:\n",
        "\n",
        "$$\\gamma(\\boldsymbol{v}) = [cos(2\\pi\\boldsymbol{B}\\boldsymbol{v}, sin(2\\pi\\boldsymbol{B}\\boldsymbol{v}))]^T$$\n",
        "\n",
        "where $\\boldsymbol{v} \\in \\mathbb{R}^2$ are the raw input pixel coordinates, and $\\boldsymbol{B} \\in \\mathbb{R}^{2 \\times D_{FF}}$ is a wide matrix that projects the low-dimensional pixel coordinates into a higher-dimensional space. The dimensionality of the space is a hyperparameter that you can choose freely. Tancik et al. recommend to set it as high as it is possible with your available memory.\n",
        "\n",
        "The entries of $\\boldsymbol{B}$ follow a normal distribution. It turns out that the mean of the normal distribution used to sample the entries of $\\boldsymbol{B}$ does not have large influence on the results. The standard deviation, however, does and is the single hyperparameter we need to tune. I set it to 10 for you in the cell below, because this value works quite well for our model and image. If you are curious, you can try out different values for the standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FOURIER_DIM = 256\n",
        "FOURIER_SCALE = 10.\n",
        "INPUT_DIMS = 2 * FOURIER_DIM\n",
        "\n",
        "B = FOURIER_SCALE * torch.randn(size=(2, FOURIER_DIM), requires_grad=False)"
      ],
      "metadata": {
        "id": "4kmKBDQnKFL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_fourier_features(x, B):\n",
        "  projection = (2 * np.pi * x) @ B\n",
        "  transformed = torch.cat([torch.sin(projection), torch.cos(projection)], dim=-1)\n",
        "  return transformed"
      ],
      "metadata": {
        "id": "pZI-XhQs8y2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now use the same model architecture as above, but apply the Fourier featuers to the input coordinates before we pass them to the model. Again, we will save a snapshot of the training progress every 25 iterations. Notice how we now reach a much higher PSNR, and a much lower loss value than before already after a few iterations!"
      ],
      "metadata": {
        "id": "hzyBKKhj886g"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUQn0jFIz3vX"
      },
      "source": [
        "model = nn.DataParallel(NeuralField(input_dimension=INPUT_DIMS).cuda())\n",
        "model.train()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "ff_images = []\n",
        "\n",
        "liveloss = PlotLosses()\n",
        "for i in range(750):\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  prediction = model(apply_fourier_features(train_X,  B))\n",
        "  loss = mse(train_y.to('cuda'), prediction)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if i % 25 == 0:\n",
        "    with torch.no_grad():\n",
        "      optimizer.zero_grad(set_to_none=True)\n",
        "      reconstruction = model(apply_fourier_features(X, B)).detach().cpu()\n",
        "\n",
        "    liveloss.update({'PSNR train': psnr(train_y, prediction.detach().cpu()),\n",
        "                     'Loss train': mse(train_y, prediction.detach().cpu()),\n",
        "                     'PSNR test': psnr(test_y, reconstruction[::2]),\n",
        "                     'Loss test': mse(test_y, reconstruction[::2])},\n",
        "                    current_step=i)\n",
        "    liveloss.send()\n",
        "    ff_images.append(reconstruction.cpu().detach().numpy().reshape(512, 512, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look again at the result that our model now produces. It looks almost identical to the original image! We only trained for 750 iterations instead of 2000 and the results look much better, all just because of the Fourier feature mapping!"
      ],
      "metadata": {
        "id": "hB1MBCrt9obV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sURUugFk2zzz"
      },
      "source": [
        "model.eval()\n",
        "predicted_image = model(apply_fourier_features(X, B).to('cuda')).cpu().detach().numpy()\n",
        "\n",
        "predicted_image = predicted_image.reshape(image.shape)\n",
        "plt.imshow(predicted_image);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, when you look at the video below, you can see that the model picks up the high frequency components of the inovex logo early. This is exactly the impact of the Fourier feature mapping on the Neural Tangent Kernel that I talk about in the blog article. The scale of the normal distribution that we use to sample the matrix $\\boldsymbol{B}$ directly influences the width of the kernel. Hence, if you play around with this hyperparameter you get different results. If the scale is too low, the kernel gets too wide and the result will look blurry. On the other hand, if the kernel gets too narrow (the scale is too large), the result will look grainy."
      ],
      "metadata": {
        "id": "baZJcX9s-NIn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b_kaqciJ7Qz"
      },
      "source": [
        "all_images = np.stack(ff_images)\n",
        "data8 = (255*np.clip(all_images,0,1)).astype(np.uint8)\n",
        "f = os.path.join('training_convergence_no_ff.mp4')\n",
        "imageio.mimwrite(f, data8, fps=10)\n",
        "\n",
        "# Display video inline\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open(f,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f'''\n",
        "<video width=500 controls autoplay loop>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWU7iFrT_ktn"
      },
      "source": [
        "## 3. Conditional Neural Fields\n",
        "\n",
        "Now that we have a Neural Field that works on single images, it is time to extend the model towards learning a whole family of images. Towards this, we will build a conditional neural field as an autodecoder and train it to reconstruct grayscale images of faces from the Yale Face Database [[Belhumeur et al.,  1997](http://vision.ucsd.edu/content/yale-face-database)].\n",
        "\n",
        "In order to speed things up and see results in orders of minutes, let us just use one subject.\n",
        "\n",
        "## Data\n",
        "\n",
        "The following few cells preprocess the data and create a PyTorch Dataset that we can use to train our model. Here a few facts about the dataset:\n",
        "\n",
        "  - The dataset comprises 165 grayscale images of 15 individuals. Hence we have 11 images per subject in different configurations.\n",
        "  - Each image is of resolution $243 \\times 320$. We will take a $240 \\times 240$ crop to train our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sKcODhSNisE"
      },
      "source": [
        "!wget -qN http://vision.ucsd.edu/datasets/yale_face_dataset_original/yalefaces.zip\n",
        "!unzip -q yalefaces.zip -d yalefaces\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgjQxq7sTR3-"
      },
      "source": [
        "example_face = imageio.imread('yalefaces/yalefaces/subject01.normal')\n",
        "\n",
        "print(example_face.shape)\n",
        "plt.imshow(example_face[2:-1, 40:-40], cmap='gray');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_Ied_65Y9ue"
      },
      "source": [
        "# sample the grid, which will be the input to the model\n",
        "x = np.linspace(0, 1, 240)\n",
        "grid = torch.tensor(np.stack(np.meshgrid(x,x), -1), dtype=torch.float32, requires_grad=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yEnCPRqrZtU"
      },
      "source": [
        "YF_PATH = Path('./yalefaces/yalefaces')\n",
        "\n",
        "all_files = np.array(list(YF_PATH.rglob('subject01*'))) # only use subject1 for this demo\n",
        "np.random.shuffle(all_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell creates a `torch.utils.data.Dataset` for our data. For simplicity, we will not use a train/test split here. In practice however, you would also make a split on pixel level here in order to spot overfitting or other problems that may occur during training!"
      ],
      "metadata": {
        "id": "l8aMnC-cBMWr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX8cTXzlPj3g"
      },
      "source": [
        "@dataclass(eq=False)\n",
        "class YaleFacesDataset(torch.utils.data.Dataset):\n",
        "  directory: Path\n",
        "  split: List[Path]\n",
        "  grid: np.ndarray\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.split)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    path = self.split[idx]\n",
        "    img = imageio.imread(path) / 255.\n",
        "    img = img[2:-1, 40:-40]\n",
        "\n",
        "    return self.grid, torch.tensor(img, dtype=torch.float32).view(1, *img.shape), idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB-LbeB8senC"
      },
      "source": [
        "ds = YaleFacesDataset(YF_PATH, split=all_files, grid=grid)\n",
        "loader = torch.utils.data.DataLoader(ds, shuffle=True, batch_size=4, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at a few examples and see if everything looks like we would expect."
      ],
      "metadata": {
        "id": "j8hLS5QlBp3k"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3Scj6tDUiwE"
      },
      "source": [
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    #img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(loader)\n",
        "grids, images, indices = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8odrfH6HkIg"
      },
      "source": [
        "### Model definition\n",
        "\n",
        "The following few cells contain different implementations of the conditioning\n",
        "techniques discussed in the blog article.\n",
        "\n",
        "We use a slightly deeper architecture than in the examples above. Namely, we will create a 4 Layer MLP.\n",
        "\n",
        "\n",
        "### Concatenation-based conditioning\n",
        "\n",
        "The following model concatenates the coordinate inputs (which we will of course pass through the Fourier feature mapping prior to passing them into the model), with the latent code for this sample along the `-1` axis. Hence, our coordinates are expected to be of shape `(N0, N1, ..., FOURIER_FEATURE_DIM)`, where `N1, N2, ...` are arbitrarily many batch dimensions, and the latent tensor should have shape `(N0, N1, ..., LATENT_DIM)`, where the batch dimensions match the ones of the coordinates tensor.\n",
        "\n",
        "The image below depitcs this conditioning method.\n",
        "\n",
        "<img src=\"https://www.inovex.de/wp-content/uploads/concat-field.drawio.png\" width=\"500px\" align=\"center\"/>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_channels, latent_size, image_shape=(240, 240)):\n",
        "        super().__init__()\n",
        "\n",
        "        # conv_theta is input convolution\n",
        "        self.conv_theta = nn.Conv2d(num_channels, 128, 3, 1, 1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 5, 2, 0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, 2, 0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, latent_size, 1, 1, 0)\n",
        "        )\n",
        "\n",
        "        self.latent_size = latent_size\n",
        "        self.fc = nn.Linear(58*58, 1)\n",
        "\n",
        "\n",
        "    def forward(self, I):\n",
        "        o = self.relu(self.conv_theta(I))\n",
        "        o = self.cnn(o)\n",
        "        o = self.fc(torch.relu(o).view(o.shape[0], self.latent_size, -1))\n",
        "        return o.squeeze(-1)\n"
      ],
      "metadata": {
        "id": "ZK58QazWEt7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalFieldConcat(nn.Module):\n",
        "  def __init__(self, num_layers=4, layer_size=128, input_dimensions=(2, 32)):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_layers = nn.ModuleList()\n",
        "    self.input_layer = nn.Linear(sum(list(input_dimensions)), layer_size)\n",
        "    for i in range(num_layers-1):\n",
        "        self.hidden_layers.append(nn.Linear(layer_size, layer_size))\n",
        "\n",
        "    self.output_layer = nn.Linear(layer_size, 1)\n",
        "\n",
        "  def forward(self, coordinate, latent):\n",
        "    cat = torch.cat([coordinate, latent], -1)\n",
        "    input = F.relu(self.input_layer(cat))\n",
        "    x = input.clone()\n",
        "    for i in range(self.num_layers -1):\n",
        "      if i == self.num_layers // 2:\n",
        "        x = x + input\n",
        "      x = F.relu(self.hidden_layers[i](x))\n",
        "\n",
        "    return torch.sigmoid(self.output_layer(x))"
      ],
      "metadata": {
        "id": "RGCK2IvOtAnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to incorporate the conditioning variable is by projecting the coordinate inputs and the latent code input to the same dimsion using two separate MLP layers and then simply add the projections. See the image below for a reference.\n",
        "<img src=\"https://www.inovex.de/wp-content/uploads/Conditional-Field.drawio.png\" width=\"500px\" align=\"center\"/>"
      ],
      "metadata": {
        "id": "7Zqes_KEDaMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalField(nn.Module):\n",
        "  def __init__(self, num_layers=4, layer_size=128, input_dimensions=(2, 32)):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.coordinate_input = nn.Linear(input_dimensions[0], layer_size)\n",
        "    self.latent_input = nn.Linear(input_dimensions[1], layer_size)\n",
        "    self.hidden_layers = nn.ModuleList()\n",
        "    for i in range(num_layers-1):\n",
        "        self.hidden_layers.append(nn.Linear(layer_size, layer_size))\n",
        "\n",
        "    self.output_layer = nn.Linear(layer_size, 1)\n",
        "\n",
        "  def forward(self, coordinate, latent):\n",
        "    x_c = F.relu(self.coordinate_input(coordinate))\n",
        "    x_l = F.relu(self.latent_input(latent))\n",
        "    x = x_c + x_l\n",
        "    for i in range(self.num_layers -1):\n",
        "      if i == self.num_layers // 2:\n",
        "        x = x + x_c\n",
        "      x = F.relu(self.hidden_layers[i](x))\n",
        "\n",
        "    return torch.sigmoid(self.output_layer(x))"
      ],
      "metadata": {
        "id": "d2d8EBmYCBzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last, but not least, we can use the Feature-wise Linear Modulation mapping, which I implemented for you in a basic version below. Note that this mechanism is slower than the other two.\n",
        "\n",
        "To explain it again, have a look at the image below. The latent vector is processed by two linear layers $\\boldsymbol{\\beta}$ and $\\boldsymbol{\\gamma}$. The two resulting feature tensors are used in an affine projection with the positional information $x$, which itself is processed by a linear layer. The output of one layer can thus be described by the formula:\n",
        "\n",
        "$\\hat{x} = L(x) \\odot \\gamma(z) + \\beta(z) $\n",
        "\n",
        "<img src=\"https://www.inovex.de/wp-content/uploads/FiLM_CBN-768x597.png\" width=\"500px\" align=\"center\"/>"
      ],
      "metadata": {
        "id": "X9ArT3muD7kq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FiLM(nn.Module):\n",
        "  def __init__(self, num_features, latent_size):\n",
        "    super().__init__()\n",
        "    self.num_features = num_features\n",
        "    self.latent_size = latent_size\n",
        "\n",
        "    self.beta = nn.Linear(latent_size, num_features)\n",
        "    self.gamma = nn.Linear(latent_size, num_features)\n",
        "    self.affine = nn.Linear(num_features, num_features)\n",
        "\n",
        "\n",
        "  def forward(self, x, z):\n",
        "    beta = self.beta(z)\n",
        "    gamma = self.gamma(z)\n",
        "\n",
        "    out = self.affine(x)\n",
        "    return gamma * out + beta\n",
        "\n",
        "class FiLMConditionedField(nn.Module):\n",
        "  def __init__(self, num_layers=4, layer_size=128, input_dimensions=(2, 32)):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.hidden_layers = nn.ModuleList()\n",
        "    self.input_layer = nn.Linear(input_dimensions[0], layer_size)\n",
        "    for i in range(num_layers-1):\n",
        "        self.hidden_layers.append(FiLM(layer_size, input_dimensions[1]))\n",
        "\n",
        "    self.output_layer = nn.Linear(layer_size, 1)\n",
        "\n",
        "  def forward(self, coordinate, latent):\n",
        "    coord = self.input_layer(coordinate)\n",
        "    x = coord.clone()\n",
        "    for i in range(self.num_layers -1):\n",
        "      if i == self.num_layers // 2:\n",
        "        x = x + coord\n",
        "      x = F.relu(self.hidden_layers[i](x, latent))\n",
        "\n",
        "    return torch.sigmoid(self.output_layer(x))"
      ],
      "metadata": {
        "id": "vTAzTP3XWONR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, lets put everything together by combining the encoder with the decoder of your choice. Feel free to ajust the code. You may need to adjust some of the hyper parameters too, if you chose another conditioning method."
      ],
      "metadata": {
        "id": "BtSwgaxoHHHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalNeuralField(nn.Module):\n",
        "\n",
        "  def __init__(self, encoder: nn.Module, decoder: nn.Module):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def encode(self, image):\n",
        "    return self.encoder(image)\n",
        "\n",
        "  def decode(self, coordinate, code):\n",
        "    return self.decoder(coordinate, code)\n",
        "\n",
        "  def forward(self, coordinate, image):\n",
        "    encoding = self.encode(image)\n",
        "    batch_dims = coordinate.shape[:-1]\n",
        "    batched_encoding = encoding.view(batch_dims[0], 1, 1, -1)\n",
        "    batched_encoding = batched_encoding.repeat(1, *batch_dims[1:], 1)\n",
        "\n",
        "    return self.decode(coordinate, batched_encoding), encoding"
      ],
      "metadata": {
        "id": "bLnULF7PHHPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtADHsViK0gE"
      },
      "source": [
        "### Training preparation\n",
        "\n",
        "Next, we have to instantiate our model, create our set of latent vectors, and create an optimizer that keeps track of both the model parameters and the latent vectors. We will also define the loss function in here. We use the per-pixel mean squared error like in the example above and add a L2 regularization term over the latent vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx3y83msyu7e"
      },
      "source": [
        "# Define some variables that we will use in the following cells\n",
        "NUM_EPOCHS = 250\n",
        "LATENT_VECTOR_SIZE =128\n",
        "LAYER_SIZE = 128\n",
        "MODEL_LR = 1e-3\n",
        "REG_WEIGHT = 1e-3\n",
        "FOURIER_DIM = 256\n",
        "FOURIER_SCALE = 10.\n",
        "INPUT_DIMS = (2 * FOURIER_DIM, LATENT_VECTOR_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft31-PYm9bJS"
      },
      "source": [
        "def loss_fn(gt, pred, vectors):\n",
        "  data_loss = mse(gt, pred)\n",
        "  regularization = torch.mean(vectors ** 2.)\n",
        "\n",
        "  return data_loss, regularization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_l0ajT7Ky1v"
      },
      "source": [
        "# feel free to change the model type here.\n",
        "encoder = Encoder(1, LATENT_VECTOR_SIZE)\n",
        "decoder = ConditionalField(input_dimensions=INPUT_DIMS, layer_size=LAYER_SIZE)\n",
        "\n",
        "model = nn.DataParallel(ConditionalNeuralField(encoder, decoder).cuda())\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    [\n",
        "     {\n",
        "      'params': model.parameters(),\n",
        "      'lr': MODEL_LR,\n",
        "     },\n",
        "])\n",
        "\n",
        "# I found the learning rate schedule to be helpful with the FiLM Mapping.\n",
        "schedule = torch.optim.lr_scheduler.StepLR(optimizer, 25)\n",
        "\n",
        "B = FOURIER_SCALE * torch.randn(size=(2, FOURIER_DIM), requires_grad=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPHL4GpT0aNO"
      },
      "source": [
        "### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del encoder, decoder\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "JksrSd0CRR-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTqjBAVS0YF-"
      },
      "source": [
        "liveloss = PlotLosses()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  losses = []\n",
        "  regs = []\n",
        "  psnrs = []\n",
        "  for X, y, idx in loader:\n",
        "    X.requires_grad = False\n",
        "    batch_size = len(idx)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    ff = apply_fourier_features(X, B)\n",
        "    ff.requires_grad = False\n",
        "\n",
        "    prediction, encoding = model(ff, y.reshape(-1, 1, 240, 240))\n",
        "\n",
        "    dloss, reg = loss_fn(y.squeeze(), prediction.squeeze().cpu(), encoding)\n",
        "    loss = dloss + REG_WEIGHT * reg\n",
        "    psnrs.append(psnr(y.squeeze(), prediction.squeeze().detach().cpu()))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "    regs.append(reg.item())\n",
        "\n",
        "  schedule.step()\n",
        "\n",
        "\n",
        "  liveloss.update({'Loss': np.mean(losses), 'L2': np.mean(regs)}, current_step=epoch)\n",
        "  liveloss.update({'PSNR': np.mean(psnrs)}, current_step=epoch)\n",
        "  liveloss.send()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHmb1kNrti_i"
      },
      "source": [
        "import torchvision as tv\n",
        "model.eval()\n",
        "\n",
        "\n",
        "X,y,_ = next(iter(loader))\n",
        "\n",
        "ff = apply_fourier_features(X, B)\n",
        "prediction, encoding = model(ff, y.reshape(-1, 1, 240, 240))\n",
        "\n",
        "pred = prediction.detach().cpu().reshape(-1, 1, 240, 240)\n",
        "\n",
        "gt = tv.utils.make_grid(y.reshape(-1, 1, 240, 240)).permute(1, 2, 0).numpy()\n",
        "pred = tv.utils.make_grid(pred).permute(1, 2, 0).numpy()\n",
        "\n",
        "stacked = np.vstack([gt, pred])\n",
        "\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "plt.imshow(stacked);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reusults do not look as clear as the reconstruction of the single image. There are multiple reasons for that. Generally, we have very little data, so it may be that the architecture struggles a bit to encode common features in the decoder and specific features in the latent codes that are generated by the encoder. Let me show you one last cool idea to condition the neural field."
      ],
      "metadata": {
        "id": "FCdhECo4ftA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Local Conditioning\n",
        "\n",
        "We can also use multiple latent codes to encode information. In this scenario, each code is tied to a spatial area on the input domain.\n",
        "\n",
        "In the following, we create a $4 \\times 4$ embedding grid with a soft assignment. We will also use the autodecoder architecture here. Therefore, we create an embedding, which holds the latent vectors for each spatial subregion and define a function that maps the vectors to the subregions for which they are responsible. For the interpolation, we build an `emb_weights` matrix, which will be of shape `(num_pixels, 16)` and contain weigths that are used later to soft-assign local latent codes to pixels. The rows of the matrix sum to one and a single pixel is assigned to a maximum of two latent codes.\n",
        "\n",
        "Just in case you wonder while reading the code below: We are using a slightly different architecture. Below, we will train the Neural Field as an autodecoder. Meaning that we do not use an encoder to create the latent encoding for us, but rather store multiple vectors in a tensor and access them by their indices. If you want a refresher on this, head over to the blog article accompanying this notebook."
      ],
      "metadata": {
        "id": "bTOWVujYHw-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SPATIAL_CODES_PER_SIDE = 4 # 4 by 4 embedding grid\n",
        "NUM_CODES = NUM_SPATIAL_CODES_PER_SIDE ** 2\n",
        "TILED_LATENT_VECTOR_SIZE = LATENT_VECTOR_SIZE // NUM_CODES\n",
        "local_embeddings = torch.nn.Embedding(len(ds) * NUM_CODES, TILED_LATENT_VECTOR_SIZE, max_norm=1.)\n",
        "\n",
        "emb_vertices, step = np.linspace(0, 1, NUM_SPATIAL_CODES_PER_SIDE, retstep=True, endpoint=False)\n",
        "emb_vertices += step/2\n",
        "\n",
        "embedding_grid = torch.tensor(\n",
        "    np.stack(np.meshgrid(emb_vertices, emb_vertices), -1),\n",
        "    dtype=torch.float32, requires_grad=False\n",
        "    )\n",
        "\n",
        "\n",
        "grid_flat = grid.reshape(-1, 2)\n",
        "emb_grid_flat = embedding_grid.reshape(-1, 2)\n",
        "\n",
        "emb_weights = torch.zeros(len(grid_flat), NUM_SPATIAL_CODES_PER_SIDE **2)\n",
        "\n",
        "for i, x in enumerate(grid_flat):\n",
        "  dists, _ = torch.max(torch.abs(x -  emb_grid_flat), -1)\n",
        "  dists[dists > step] = 0\n",
        "\n",
        "\n",
        "  emb_weights[i, :] = dists / torch.sum(dists)"
      ],
      "metadata": {
        "id": "Y_3WVNkJOeq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we create the model and optimizer. As we are in the autodecoder setting now, we need to also assign the `local_embedding` tensor to our optimizer, so that the values will get updated during training."
      ],
      "metadata": {
        "id": "4OsiuUvEGaeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIMS = (2 * FOURIER_DIM, TILED_LATENT_VECTOR_SIZE)\n",
        "LATENT_LR = 1e-4\n",
        "\n",
        "model = nn.DataParallel(ConditionalFieldConcat(input_dimensions=INPUT_DIMS, layer_size=LAYER_SIZE).cuda())\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    [\n",
        "     {\n",
        "      'params': model.parameters(),\n",
        "      'lr': MODEL_LR,\n",
        "     },\n",
        "     {\n",
        "      'params': local_embeddings.parameters(),\n",
        "      'lr': LATENT_LR\n",
        "     }\n",
        "])\n",
        "\n",
        "# I found the learning rate schedule to be helpful with the FiLM Mapping.\n",
        "schedule = torch.optim.lr_scheduler.StepLR(optimizer, 40)\n",
        "\n",
        "B = FOURIER_SCALE * torch.randn(size=(2, FOURIER_DIM), requires_grad=False)"
      ],
      "metadata": {
        "id": "7Yw8QyTUIWjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function builds the vector tensor that we pass to our model. Instead of using a single, global vector for each pixel coordinate, we will use the `emb_weight` matrix to perform the soft assignment."
      ],
      "metadata": {
        "id": "_chuwssGGs-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vectors(indices):\n",
        "  vectors = []\n",
        "  raw = []\n",
        "  for idx in indices:\n",
        "    embs = local_embeddings(torch.arange(idx * NUM_CODES, idx * NUM_CODES + NUM_CODES, dtype=torch.int32))\n",
        "    raw.append(embs)\n",
        "    vectors.append(emb_weights @ embs)\n",
        "\n",
        "  vectors = torch.stack(vectors)\n",
        "  raw = torch.stack(raw)\n",
        "  return vectors.reshape(len(indices), *grid.shape[:2], -1), raw\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "A14fOw08EV7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mnSMT1wThhLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training\n",
        "\n",
        "It's time to train this architecture. It takes a few more iterations than the other methods and thus, a few minutes more. Just that you aware ;)"
      ],
      "metadata": {
        "id": "KG5rulDvhhVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "liveloss = PlotLosses()\n",
        "\n",
        "for epoch in range(500):\n",
        "  losses = []\n",
        "  psnrs = []\n",
        "  for X, y, idx in loader:\n",
        "    X.requires_grad = False\n",
        "    batch_size = len(idx)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    ff = apply_fourier_features(X, B)\n",
        "    ff.requires_grad = False\n",
        "\n",
        "    vectors, vraw = build_vectors(idx)\n",
        "    prediction = model(ff, vectors)\n",
        "\n",
        "    data_loss, reg = loss_fn(y.squeeze(), prediction.squeeze().cpu(), vraw)\n",
        "    loss = data_loss + REG_WEIGHT * reg\n",
        "    psnrs.append(psnr(y.squeeze(), prediction.squeeze().detach().cpu()))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "\n",
        "  #schedule.step()\n",
        "\n",
        "  liveloss.update({'Loss': np.mean(losses)}, current_step=epoch)\n",
        "  liveloss.update({'PSNR': np.mean(psnrs)}, current_step=epoch)\n",
        "  liveloss.send()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o633J0rbLTO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uff, that took a while! But you should see from the loss curve that the model indeed did learn something and is able to create images of faces from the local conditioning. Let us have a look at some of the results."
      ],
      "metadata": {
        "id": "XQ1sQH4r0l-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "random_idx = torch.tensor(np.random.choice(len(ds), 4))\n",
        "images = []\n",
        "for idx in random_idx:\n",
        "  embs = local_embeddings(torch.arange(idx * NUM_CODES, idx * NUM_CODES + NUM_CODES, dtype=torch.int32))\n",
        "  emb = (emb_weights @ embs).reshape(1, *grid.shape[:2], -1)\n",
        "\n",
        "  ff = apply_fourier_features(grid, B)\n",
        "\n",
        "  pred = model(ff.unsqueeze(0), emb).squeeze().detach().cpu().numpy()\n",
        "  images.append(pred)\n",
        "\n",
        "concat = np.concatenate(images, axis=1)\n",
        "fig = plt.figure(figsize=(12,3))\n",
        "plt.imshow(concat, cmap='gray');"
      ],
      "metadata": {
        "id": "CfZs4_hNOd-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even after 500 epochs of training, you can see some of the boundaries between the patches. This indicates that our interpolation between the patches is maybe not ideal. If you want to, you can use the code above and play with it a little bit to see whether you can create a better result.\n",
        "\n",
        "## Final remark\n",
        "\n",
        "So you worked your way through this quite lentghy notebook, congratulations! I hope you learned something and enjoyed this content. At least I did while creating this notebook and I definitely will proceed working on and with Neural Fields, as I truly believe they have the potential to do amazing things at the interesection of Deep Learning and Physics, or Deep Learning and Arts. If you enjoyed the notebook, feel free to share it with your peers."
      ],
      "metadata": {
        "id": "C2LcNG0v0PHN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29Fhxumd02Dp"
      },
      "source": [
        "---\n",
        "\n",
        "<img src=\"https://www.inovex.de/wp-content/uploads/inovex-logo-dunkelblau-quadrat.png\" width=\"100px\" align=\"left\">\n",
        "<br /><br /><br /><br /><br />\n",
        "\n",
        "\n",
        "Find more notebooks like this on our <a href=\"https://github.com/inovex/notebooks\">Github</a>.\n",
        "\n",
        "For in-depth articles have a look at our [blog](https://www.inovex.de/blog).\n",
        "\n",
        "Shared with 💙 by [inovex.de](https://www.inovex.de)."
      ]
    }
  ]
}